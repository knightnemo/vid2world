<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Vid2World: Crafting Video Diffusion Models to Interactive World Models">
  <meta name="keywords" content="Vid2World, World Model, Video Diffusion Model">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Vid2World: Crafting Video Diffusion Models to Interactive World Models</title>

  <!-- Add favicon -->
  <link rel="icon" type="image/x-icon" href="./static/images/logo.png">
  <!-- For modern browsers -->
  <link rel="icon" type="image/png" href="./static/images/logo.png">

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" target="_blank" href="https://knightnemo.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" target="_blank" href="https://thuml.github.io/iVideoGPT/">
            iVideoGPT
          </a>
          <a class="navbar-item" target="_blank" href="https://arxiv.org/abs/2502.01366">
            TrajWorld
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          
          <h1 class="title is-1 publication-title"><img src="static/images/logo.png" alt="Logo" style="height: 1em; vertical-align: top; margin-right: 0em;"> Vid2World: Crafting Video Diffusion Models to Interactive World Models</h1>
          <!-- <h3 class="title is-4">NeurIPS 2024</h3> -->

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://knightnemo.github.io/">Siqiao Huang</a>*<sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://manchery.github.io/">Jialong Wu</a>*<sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://github.com/potatoQi">Qixing Zhou</a><sup>1,3</sup>,</span>
            <span class="author-block">
              <a href="https://openreview.net/profile?id=~Shangchen_Miao1">Shangchen Miao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="http://ise.thss.tsinghua.edu.cn/~mlong/">Mingsheng Long</a>#<sup>1</sup>
            </span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup>School of Software, BNRist, Tsinghua University,</span>
            <span class="author-block"><sup>2</sup>IIIS, Tsinghua University</span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>3</sup>College of Computer Science, Chongqing University,</span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block">* Equal Contribution,</span>
            <span class="author-block"># Corresponding Author</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a target="_blank" href="TBD"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              
              <!-- Code Link. -->
               <!--
              <span class="link-block">
                <a target="_blank" href="https://github.com/thuml/iVideoGPT"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              -->
              <!-- Model Link. -->
              <!--

              <span class="link-block">
                <a target="_blank" href="https://huggingface.co/collections/thuml/ivideogpt-674c59cae32231024d82d6c5"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-database"></i>
                  </span>
                  <span>Models</span>
                  </a>
              </span>
              -->
              <!-- Poster Link. -->
              <!--
              <span class="link-block">
                <a target="_blank" href="https://manchery.github.io/assets/pub/nips2024_ivideogpt/poster.pdf"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-image"></i>
                  </span>
                  <span>Poster</span>
                  </a>
              </span>
              -->
              <!-- Slides Link. -->
              <!--
              <span class="link-block">
                <a target="_blank" href="https://manchery.github.io/assets/pub/nips2024_ivideogpt/slides.pdf"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-film"></i>
                  </span>
                  <span>Slides</span>
                  </a>
              </span>
              -->
              <span class="link-block">
                <a href="TBD"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Blog Link. -->
              <!--
              <span class="link-block">
                <a target="_blank" href="https://mp.weixin.qq.com/s/D94aamdqtO9WLekr4BSCUw"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fa fa-book"></i>
                  </span>
                  <span>Blog</span>
                  </a>
              </span>
            -->
            </div>
            <div class="is-size-5 mt-3">
              <span class="has-text-weight-bold">TL;DR:</span> Vid2World is a general approach for transforming 
              video diffusion models (like SORA) into interactive world models (like Genie), 
              leveraging the high fidelity of full-sequence diffusion to enable causal, 
              autoregressive, and action-conditioned generation.
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <p style="margin-bottom: 0.5rem; text-align: left; margin-left: 6%;">
        <b>Generation Result of Vid2World.</b> We show the generation results of Vid2World in the videos below.
      </p>
      <div class="carousel results-carousel">
        <div class="item-2">
          <video poster="" id="rollout-1" autoplay controls muted loop playsinline height="100%" loading="lazy">
            <source src="./static/csgo/hdf5_dm_july2021_2554/pred_video.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item-2">
          <video poster="" id="rollout-1" autoplay controls muted loop playsinline height="100%" loading="lazy">
            <source src="./static/csgo/hdf5_dm_july2021_2888/pred_video.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item-2">
          <video poster="" id="rollout-1" autoplay controls muted loop playsinline height="100%" loading="lazy">
            <source src="./static/appendix/1/pred_video.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item-2">
          <video poster="" id="rollout-2" autoplay controls muted loop playsinline height="100%" loading="lazy">
            <source src="./static/appendix/2/pred_video.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item-2">
          <video poster="" id="rollout-3" autoplay controls muted loop playsinline height="100%" loading="lazy">
            <source src="./static/appendix/3/pred_video.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item-2">
          <video poster="" id="rollout-4" autoplay controls muted loop playsinline height="100%" loading="lazy">
            <source src="./static/appendix/4/pred_video.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item-2">
          <video poster="" id="rollout-2" autoplay controls muted loop playsinline height="100%" loading="lazy">
            <source src="./static/appendix/5/pred_video.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item-2">
          <video poster="" id="rollout-2" autoplay controls muted loop playsinline height="100%" loading="lazy">
            <source src="./static/appendix/6/pred_video.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item-2">
          <video poster="" id="rollout-2" autoplay controls muted loop playsinline height="100%" loading="lazy">
            <source src="./static/appendix/7/pred_video.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item-2">
          <video poster="" id="rollout-2" autoplay controls muted loop playsinline height="100%" loading="lazy">
            <source src="./static/appendix/8/pred_video.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item-2">
          <video poster="" id="rollout-1" autoplay controls muted loop playsinline height="100%" loading="lazy">
            <source src="./static/csgo/hdf5_dm_july2021_327/pred_video.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item-2">
          <video poster="" id="rollout-1" autoplay controls muted loop playsinline height="100%" loading="lazy">
            <source src="./static/csgo/hdf5_dm_july2021_1018/pred_video.mp4" type="video/mp4">
          </video>
        </div>
        <div class="item-2">
          <video poster="" id="rollout-1" autoplay controls muted loop playsinline height="100%" loading="lazy">
            <source src="./static/csgo/hdf5_dm_july2021_1865/pred_video.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <hr style="border: none; border-top: 1px dashed #9c9c9c; width: 100%;">
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">

    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            World models, which predict transitions based on history observation and action sequences, 
            have shown great promise in improving data efficiency for sequential decision making. 
            However, existing world models often require extensive domain-specific training 
            and still produce low-fidelity, coarse predictions, limiting their applicability in complex environments.
             In contrast, video diffusion models trained on large, internet-scale datasets have 
             demonstrated impressive capabilities in generating high-quality videos that 
             capture diverse real-world dynamics. In this work, we present <b>Vid2World</b>, 
             a general approach for leveraging and transferring pre-trained video diffusion models into interactive world models. 
             To bridge the gap, Vid2World performs <b>casualization</b> of a pre-trained video diffusion model 
             by crafting its architecture and training objective to enable autoregressive generation. 
             Furthermore, it introduces a <b>causal action guidance</b> mechanism to enhance action 
             controllability in the resulting interactive world model. Extensive experiments in robot manipulation 
             and game simulation domains show that our method offers a scalable and effective approach for repurposing 
             highly capable video diffusion models to interactive world models.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>

<div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
    <div class="column is-full-width has-text-justified">
      <h2 class="title is-3 has-text-centered">🎥 Vid2World 🌍</h2>
      <div class="content has-text-justified">
        <p>
          <strong>Vid2World</strong> is a general framework for 
          transforming full-sequence, non-causal, passive video diffusion models
          into autoregressive, interactive, action-conditioned world models.
        </p>
        <p>
          Transforming a video diffusion model to an interactive world model demands solving two fundamental gaps:
           (i) Enabling causal generation—standard VDMs denoise whole sequences with bidirectional context, 
           making it unsuitable for causal rollouts where predictions only
           depend on past information; (ii) Enforcing action conditioning—VDMs are usually conditioned 
           only on coarse, video-level inputs (e.g. text, image), lacking the ability to perform frame-level fine-grained
           action-conditioned predictions. 
        </p>
      </div>
      <br>
      <div class="content has-text-centered">
        <img src="./static/images/concept.png" class="interpolation-image" width="100%" alt="Concept of VDM to IWM transformation.">
      </div>
      <div class="content has-text-justified">
        <p>
          <b>Video Diffusion Causalization.</b> Vid2World re-architects the pretrained diffusion backbone 
          so that each frame is generated from past context only. 
          From the architectural perspective, it applies causal masks to temporal attention layers, and proposed a mixed weight transfer scheme for temporal convolution layers.
          From the training perspective, to enable auto-regressive generation capabilities, 
          it samples independent and uniform
          noise levels in different frames, following <a href="https://arxiv.org/abs/2407.01392">Diffusion Forcing</a>.
          These changes retain the model's learned visual priors while enabling unlimited-horizon, stepwise causal rollout.
        </p>
      </div>
      <br>
      <div class="content has-text-centered">
        <img src="./static/images/mwt.png" class="interpolation-image" width="50%" alt="Mixed Weight Transfer.">
      </div>
      <br>
      <div class="content has-text-justified">
        <p>
          <b>Causal Action Guidance.</b> To make the causal model responsive to fine-grained actions, 
          Vid2World extends classifier-free guidance to sequential settings. 
          Each action is encoded with a lightweight MLP and added at its corresponding frame; 
          during training the action is independently dropped with fixed probability, 
          forcing the network to learn both conditional and unconditional score functions. 
          At test time these the conditional score function and it's unconditional counterpart with the most recent action dropped out are 
          linearly combined with a tunable guidance scale λ, 
          offering test-time flexibility on the responsiveness
          to fine-grained action variations.
        </p>
      </div>
      <br>
      <div class="content has-text-centered">
        <img src="./static/images/method.png" class="interpolation-image" width="100%" alt="Training and Inference of Vid2World.">
      </div>
      <br>
      <div class="content has-text-justified">
        <p>
          As a result, Vid2World unifies the high-fidelity synthesis 
          of full-sequence diffusion with the causal reasoning and controllability 
          of interactive world models, delivering high-fidelity, precise, and action-aware predictions 
          in an auto-regressive manner.
        </p>
        <br>
      </div>
    </div>
  </div>
</div>
<div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
    <div class="column is-full-width has-text-justified">
      <h2 class="title is-3 has-text-centered"> 🤖 Vid2World for Robot Manipulation 🦾</h2>
      <div class="content has-text-justified">
        <p>
          Here we utilize the <a href="https://robotics-transformer1.github.io">RT-1 dataset</a>. We provide a list of videos generated by Vid2World along side the ground truth videos (denoted as GT). The 
          generated videos are given the first frame and a action sequence, 
          rolled out in an auto-regressive manner.
        </p>
      </div>

      <div class="publication-video">
        <video id="dmlab" autoplay="" muted="" loop="" playsinline="" width="100%">
          <source src="static/rt1/combined/all_combined.mp4" type="video/mp4">
        </video>
      </div>
      <div class="content has-text-justified" style="margin-top: 1rem;">
        <p>
          As shown in the videos, Vid2World is capable of generating predictions that closely 
          resemble the ground truth sequences, both in terms of visual fidelity and dynamic consistency.
        </p>
        <p>
          To demonstrate our method's capabilities to aid downstream tasks in interactive environments,
          we conduct Real2Sim Policy Evaluation experiments, 
          following <a href="http://arxiv.org/abs/2405.05941">SIMPLER</a>.
          Here, given an initial frame, the predefined policies interact with the world model interactively, rolling out 
          trajectories within the world model. We use the <code>close_drawer</code> task, where the goal is to close the drawer, 
          and the policies are three checkpoints of RT-1 at different stages of training.
        </p>
      </div>

      <div class="content has-text-centered">
        <!-- <img
          src="./static/images/dmlab_df_0.png"
          class="inline-figure-six"
          height="auto"
          width="100%" />
        <hr /> -->

        <img src="./static/images/ope-vis.png" class="inline-figure-six" height="auto" width="80%">
      </div>
      <div class="content has-text-justified" style="margin-top: 1rem;">
        <p>
          As shown in the figure, Vid2World reliably captures and reflects the behavioral differences 
          and performance variations among different policies, demonstrating its 
          capability to distinguish successful and failed executions through autoregressive rollouts.
        </p>  
        <br>
      </div>
    </div>
  </div>
</div>
<div class="container is-max-desktop">
  <div class="columns is-centered has-text-centered">
    <div class="column is-full-width has-text-justified">
      <h2 class="title is-3 has-text-centered">🎮 Vid2World for Game Simulation 🕹️</h2>
      <div class="content has-text-justified">
        <p>
          Here we adopt the celebrated game of <a href="https://arxiv.org/abs/2104.04258">CS:GO</a>. We provide a list of videos generated by Vid2World along side the ground truth videos (denoted as GT). To make the 
          generated videos comparable to baselines, we provide the first four frames and a action sequence, and the videos are
          generated in an auto-regressive manner. As demonstrated in the videos, Vid2World generates predictions 
          that closely align with the ground truth sequences, 
          exhibiting high visual fidelity and consistent temporal dynamics.
        </p>
      </div>

      <div class="publication-video">
        <video id="dmlab" autoplay="" muted="" loop="" playsinline="" width="100%">
          <source src="static/csgo/combined/all_combined.mp4" type="video/mp4">
        </video>
      </div>
      <div class="content has-text-justified" style="margin-top: 1rem;">
        <p>
          To showcase our model's capability of counterfactual video generation with the current action, instead of predicting trends based solely on past
          observations, we the following video, where all trajectories start from the same observation, but leading to completely different
          generated frame sequences in cause of different action sequences.
        </p>
        <div class="publication-video">
          <video id="dmlab" autoplay="" muted="" loop="" playsinline="" width="100%">
            <source src="./static/move/combined_video.mp4" type="video/mp4">
          </video>
        </div>
        <p>
          To further validate the effectiveness of our method, we showcase examples of Vid2World's predictions compared
          to the strong baseline of <a href="https://diamond-wm.github.io">DIAMOND</a>, under two configurations.
        </p>
        <p>
          <b>Error Accumulation.</b> A common issue with auto-regressive generation is error accumulation, 
          where errors in early frames are amplified as the generation progresses.
          Here, we show a comparison between Vid2World and DIAMOND, where the former is able to provide 
          temporally consistent predictions, while the latter accumulates errors, resulting in a progressively blurred video.
        </p>
      </div>
      <div class="publication-video">
        <video id="dmlab" autoplay="" muted="" loop="" playsinline="" width="100%">
          <source src="./static/compare/comparison/18_comparison.mp4" type="video/mp4">
        </video>
      </div>
      <div class="content has-text-justified" style="margin-top: 1rem;">
        <p>
          <b>Action Alignment.</b> The reliability of a world model, to a large extent, 
          depends on how well its
          predictions align with the input actions. 
          Vid2World accurately reflects the 
          <i>aim-down-sights</i> action in its predicted video, 
          whereas Diamond fails to manifest this action.
        </p>
      </div>
      <div class="publication-video">
        <video id="dmlab" autoplay="" muted="" loop="" playsinline="" width="100%">
          <source src="./static/compare/comparison/81_comparison.mp4" type="video/mp4">
        </video>
      </div>
      <div class="content has-text-justified" style="margin-top: 1rem;">
        <p>
          <b>Limitations.</b> Despite substantially reducing accumulated error and preserving action alignment,
          Vid2World still encounters failure cases, demonstrated in Figure 10. In this figure, neither Vid2World
          nor Diamond matches the ground truth. Although the model's capability is one important factor
          leading to failure, the environment's randomness, in this case the place for player's respawn also adds
          to the difficulty
        </p>
      </div>
      <div class="publication-video">
        <video id="dmlab" autoplay="" muted="" loop="" playsinline="" width="100%">
          <source src="./static/compare/comparison/92_comparison.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </div>
</div>


<section class="section" id="citation">
  <div class="container is-max-desktop content">
    <h2 class="title">Citation</h2>
    <pre><code>Placeholder</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" 
         href="Coming Soon">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="Coming Soon" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
      <!-- <a class="icon-link" href="https://huggingface.co/thuml/ivideogpt-oxe-64-act-free" class="external-link" disabled>
        <i class="fa fa-database"></i>
      </a>
      <a class="icon-link"
         href="https://manchery.github.io/">
        <i class="fas fa-home"></i>
      </a> -->
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website template is modified from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
// 立即执行函数，不等待DOMContentLoaded
(function() {
  function playVideo(video) {
    video.muted = true;
    video.play().catch(function(error) {
      console.log("Auto-play was prevented");
      // 如果播放失败，尝试使用用户交互来触发播放
      video.addEventListener('click', function() {
        video.play();
      }, { once: true });
    });
  }

  // 立即处理所有视频
  const videos = document.getElementsByTagName('video');
  for (let video of videos) {
    playVideo(video);
  }

  // 监听DOM变化，处理新添加的视频
  const observer = new MutationObserver(function(mutations) {
    mutations.forEach(function(mutation) {
      mutation.addedNodes.forEach(function(node) {
        if (node.nodeName === 'VIDEO') {
          playVideo(node);
        }
      });
    });
  });

  observer.observe(document.body, { childList: true, subtree: true });

  // 监听轮播变化
  const carousels = document.querySelectorAll('.carousel');
  carousels.forEach(carousel => {
    carousel.addEventListener('slide:change', function() {
      const carouselVideos = carousel.getElementsByTagName('video');
      for (let video of carouselVideos) {
        playVideo(video);
      }
    });
  });
})();
</script>

</body>
</html>
